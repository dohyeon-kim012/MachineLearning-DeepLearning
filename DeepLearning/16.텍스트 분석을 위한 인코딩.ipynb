{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "16. 텍스트 분석을 위한 인코딩",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOnXD+lo0IA2eaplsb6j0fd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dohyeon-kim012/MachineLearning-DeepLearning/blob/main/DeepLearning/16.%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D%EC%9D%84%20%EC%9C%84%ED%95%9C%20%EC%9D%B8%EC%BD%94%EB%94%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fqs2oB2Hif3"
      },
      "source": [
        "# 텍스트의 수치화\n",
        "컴퓨터는 텍스트보다 숫자를 더 잘 처리한다. 텍스트를 수치화 시키는 작업에 대해 알아봄\n",
        "1. **Integer Encoding ( 정수 인코딩 )**\n",
        "  * 단어 토큰화( Word Tokenization ) 또는 형태소 분리 후에 각 단어에 대한 **고유한 정수**를 부여\n",
        "  * 중복이 허용되지 않는 모든 단어들의 집합을 만들어야 한다. **단어 집합(Vocabulary)**이라고 한다\n",
        "  - OOV : UNK(unknown) 토큰이라고도 함  \n",
        "  단어 집합에 포함되지 않은 단어들을 정수화하기 위한 토큰으로, 주로 집합 내의 앞쪽 ( 0번 / 1번 ) 에 위치함\n",
        "2. Padding\n",
        "  * 모든 문장에 대해서 정수 인코딩을 했을 때 문장마다의 길이가 다를 수 있다.\n",
        "  * 이때 가상의 단어( 예를 들면 `<pad>` )를 만들어서 단어집합에 0번으로 추가시켜 준다. 이를 Padding 작업이라고 한다.\n",
        "  * 짧은 길이의 문장의 제일 앞이나 뒤에 `<pad>` 정수를 추가\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QBJhR5CH6se"
      },
      "source": [
        "## Integer Encoding\n",
        "\n",
        "토큰에 고유한 정수를 부여하는 작업\n",
        "\n",
        "숫자를 부여하는 기준은 ABC ( 가나다 ) 순 또는 빈도가 높은 순으로 순서 구성\n",
        "- 많이 등장한 단어일수록 작은 정수를 부여받는 식"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8mbg3eAJQzk",
        "outputId": "27ade686-143a-4d54-8a3c-deb4706ee79b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDO3BB-AJTfV"
      },
      "source": [
        "### Encoding 전 단계"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKkutQFPbKkV"
      },
      "source": [
        "#### [English] Sentence Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA7DC2ClJi5E"
      },
      "source": [
        "text = \"\"\"Isn't she lovely.\n",
        "Isn't she wonderful.\n",
        "Isn't she precious.\n",
        "Less than one minute old.\n",
        "I never thought through love we'd be.\n",
        "Making one as lovely as she.\n",
        "But isn't she lovely made from love.\n",
        "Isn't she pretty.\n",
        "Truly the angel's best.\n",
        "Boy, I'm so happy.\n",
        "We have been heaven blessed.\n",
        "I can't believe what God has done.\n",
        "Through us he's given life to one.\n",
        "But isn't she lovely made from love.\n",
        "Isn't she lovely.\n",
        "Life and love are the same.\n",
        "Life is Aisha.\n",
        "The meaning of her name.\n",
        "Londie, it could have not been done.\n",
        "Without you who conceived the one.\n",
        "That's so very lovely made from love.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHHimnb1JkR8",
        "outputId": "25e014d8-d6b5-44fa-e15a-881170802604"
      },
      "source": [
        "# Sentence Tokenization\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sent_tokens = sent_tokenize(text)\n",
        "print(sent_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Isn't she lovely.\", \"Isn't she wonderful.\", \"Isn't she precious.\", 'Less than one minute old.', \"I never thought through love we'd be.\", 'Making one as lovely as she.', \"But isn't she lovely made from love.\", \"Isn't she pretty.\", \"Truly the angel's best.\", \"Boy, I'm so happy.\", 'We have been heaven blessed.', \"I can't believe what God has done.\", \"Through us he's given life to one.\", \"But isn't she lovely made from love.\", \"Isn't she lovely.\", 'Life and love are the same.', 'Life is Aisha.', 'The meaning of her name.', 'Londie, it could have not been done.', 'Without you who conceived the one.', \"That's so very lovely made from love.\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTFr4qPeJt11"
      },
      "source": [
        "#### [English] Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWyq0xfGJ7Cj",
        "outputId": "a891e325-28c6-41c4-bc61-1dbe58245900"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p5Oc5lgJ_bN"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words('english')) # 불용어 집합화"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mupL9EcKS9k"
      },
      "source": [
        "위의 문장이 단어 토큰화 되면 아래와 같은 2차원의 배열을 이루게 됨\n",
        "\n",
        "```python\n",
        " [[\"Isn't\", \"She\", \"lovely\"],\n",
        "  [\"Isn't\", \"she\", \"wonderful\"]]\n",
        "```\n",
        "\n",
        "리스트를 먼저 만들어 `for` 문을 통해 단어 토큰화 시키기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TX4YNiTxLJZj",
        "outputId": "042545ae-5e46-41af-da5f-79ce277031e8"
      },
      "source": [
        "# 단어 토큰화된 전체 문장을 가지고 있을 리스트\n",
        "sentences = []\n",
        "\n",
        "# 문장을 하나씩 꺼내서 단어 토큰화시키기\n",
        "for sent in sent_tokens:\n",
        "    # print(sent)\n",
        "    word_tokens = word_tokenize(sent) # 각 문장에 대한 단어 토큰화\n",
        "    # print(word_tokens)\n",
        "    result = [] # 정제 작업이 완료된 단어를 추가할 배열 ( 불용어 처리, 길이가 짧은 단어 등 제외 처리 )\n",
        "\n",
        "    for word in word_tokens:\n",
        "        # 모든 단어를 소문자화 ( -> 정규화 normalization )\n",
        "        word = word.lower() \n",
        "\n",
        "        # 불용어 처리\n",
        "        if word not in stop_words:\n",
        "\n",
        "            # 단어의 길이가 2 초과인 것만 사용하기\n",
        "            if len(word) > 2:\n",
        "                result.append(word)\n",
        "        \n",
        "    sentences.append(result)\n",
        "\n",
        "# print(result)\n",
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[\"n't\", 'lovely'], [\"n't\", 'wonderful'], [\"n't\", 'precious'], ['less', 'one', 'minute', 'old'], ['never', 'thought', 'love'], ['making', 'one', 'lovely'], [\"n't\", 'lovely', 'made', 'love'], [\"n't\", 'pretty'], ['truly', 'angel', 'best'], ['boy', 'happy'], ['heaven', 'blessed'], [\"n't\", 'believe', 'god', 'done'], ['given', 'life', 'one'], [\"n't\", 'lovely', 'made', 'love'], [\"n't\", 'lovely'], ['life', 'love'], ['life', 'aisha'], ['meaning', 'name'], ['londie', 'could', 'done'], ['without', 'conceived', 'one'], ['lovely', 'made', 'love']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vYqMXIlNQ_s"
      },
      "source": [
        "for 문이 돌 때마다 result는 계속 새로 만들어짐  \n",
        "--> 가장 마지막에 result를 프린트 하면 마지막 문장의 토큰화된 단어들만 나옴  \n",
        "`['lovely', 'made', 'love']`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hDtgj9_LWlc"
      },
      "source": [
        "#### [English] 단어 집합 만들기 ( Python )\n",
        "\n",
        "규칙 : 등장한 횟수가 많은 단어 일수록 앞 번호에 위치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acmRvCABNvzY",
        "outputId": "7d4d4b92-2d78-4af1-c52b-15a99e0a47a4"
      },
      "source": [
        "# 1. 문장 별로 모여있는 단어들을 풀어서 합치기 \n",
        "#     배열의 확장이 필요!! (2차원 배열 -> 1차원 배열)\n",
        "\n",
        "words = sum(sentences, []) \n",
        "# sentences에 들어있는 모든 배열을 비어있는 배열(새로운 리스트)에 합치는 역할 (확장 extends 됨)\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"n't\", 'lovely', \"n't\", 'wonderful', \"n't\", 'precious', 'less', 'one', 'minute', 'old', 'never', 'thought', 'love', 'making', 'one', 'lovely', \"n't\", 'lovely', 'made', 'love', \"n't\", 'pretty', 'truly', 'angel', 'best', 'boy', 'happy', 'heaven', 'blessed', \"n't\", 'believe', 'god', 'done', 'given', 'life', 'one', \"n't\", 'lovely', 'made', 'love', \"n't\", 'lovely', 'life', 'love', 'life', 'aisha', 'meaning', 'name', 'londie', 'could', 'done', 'without', 'conceived', 'one', 'lovely', 'made', 'love']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB9h6MJpODti",
        "outputId": "41ebdc40-4fca-4caa-bc6a-044171dae499"
      },
      "source": [
        "# 2. 배치를 위해 빈도수 구하기\n",
        "from collections import Counter # 횟수 세기 용도\n",
        "vocab = Counter(words) # 배열 내 단어들의 모든 빈도를 손쉽게 계산 가능\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({\"n't\": 8, 'lovely': 6, 'love': 5, 'one': 4, 'made': 3, 'life': 3, 'done': 2, 'wonderful': 1, 'precious': 1, 'less': 1, 'minute': 1, 'old': 1, 'never': 1, 'thought': 1, 'making': 1, 'pretty': 1, 'truly': 1, 'angel': 1, 'best': 1, 'boy': 1, 'happy': 1, 'heaven': 1, 'blessed': 1, 'believe': 1, 'god': 1, 'given': 1, 'aisha': 1, 'meaning': 1, 'name': 1, 'londie': 1, 'could': 1, 'without': 1, 'conceived': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0a_Ytt8Oj_6",
        "outputId": "e6c3cab3-045d-439b-9c56-0f58a7116cc8"
      },
      "source": [
        "# lovely는 몇 번 나왔나?\n",
        "vocab['lovely']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veMSdCjHPEC2"
      },
      "source": [
        "가나다 순 또는 **빈도 내림차순**으로 정렬하여 각 단어에 중복되지 않는 정수를 부여"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9XiEr6gPip2",
        "outputId": "01d31dde-c75b-4878-a89b-493ef45b0694"
      },
      "source": [
        "# 빈도수가 높은 순서대로 정렬해서 리스트에 넣어 두기\n",
        "# dict의 items를 뽑아서 value로 내림차순 정렬해야 함\n",
        "vocab_sorted = sorted(vocab.items(), key=lambda x : x[1], reverse=True) \n",
        "# key : 정렬한 기준 (여기서 x[1]는 value 의미)\n",
        "# 내림차순이니까 reverse=True\n",
        "# sorted : 리스트를 정렬\n",
        "print(vocab_sorted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(\"n't\", 8), ('lovely', 6), ('love', 5), ('one', 4), ('made', 3), ('life', 3), ('done', 2), ('wonderful', 1), ('precious', 1), ('less', 1), ('minute', 1), ('old', 1), ('never', 1), ('thought', 1), ('making', 1), ('pretty', 1), ('truly', 1), ('angel', 1), ('best', 1), ('boy', 1), ('happy', 1), ('heaven', 1), ('blessed', 1), ('believe', 1), ('god', 1), ('given', 1), ('aisha', 1), ('meaning', 1), ('name', 1), ('londie', 1), ('could', 1), ('without', 1), ('conceived', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WU_EklMQbay"
      },
      "source": [
        "높은 빈도수를 가진 단어일수록 낮은 정수 인덱스 부여 ( 앞쪽에 배치 )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsBYG88BQsoU",
        "outputId": "decb1243-3a06-4de5-b662-f6a014899ac2"
      },
      "source": [
        "word2idx = {}\n",
        "i = 0\n",
        "\n",
        "for (word, frequency) in vocab_sorted:\n",
        "\n",
        "    # 정제 작업 : 빈도수가 적은 단어 제외\n",
        "    if frequency > 1:\n",
        "        i = i+1\n",
        "        word2idx[word] = i # dict에 단어와 정수 추가\n",
        "\n",
        "print(word2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"n't\": 1, 'lovely': 2, 'love': 3, 'one': 4, 'made': 5, 'life': 6, 'done': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEGpaSGZRFbz"
      },
      "source": [
        "★★★★빈도수가 가장 높은 **상위 n개**만 선택해서 단어집합으로 사용하기★★★★\n",
        "\n",
        "단어 집합의 크기 / 단어 집합에서 사용할 단어의 개수  \n",
        "`vocab_size = 5`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8tN-1hKRlDY",
        "outputId": "bebe9847-396c-4ca1-e1bd-53d8c57db1cf"
      },
      "source": [
        "vocab_size = 5\n",
        "\n",
        "# 포함되지 않을 단어 구하기\n",
        "#   idx가 vocab_size(5)보다 초과된 것들만 삭제하는 단어에 넣기\n",
        "del_word = [w for w, idx in word2idx.items() if idx >= vocab_size +1]\n",
        "print(del_word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['life', 'done']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yVybErsSEaz",
        "outputId": "2cbf21e9-1512-41e3-8f15-f0a9f08a7805"
      },
      "source": [
        "for w in del_word:\n",
        "    del word2idx[w] # del 함수 이용하여 데이터 지우기\n",
        "\n",
        "print(word2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"n't\": 1, 'lovely': 2, 'love': 3, 'one': 4, 'made': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHw9aGeuV9Ii"
      },
      "source": [
        "### Encoding 수행하기\n",
        "\n",
        "인코딩 : 단어를 숫자로 표현하는 것"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-zASPXeXWAL",
        "outputId": "58e1b5a6-7d26-449b-8940-fa3ad996fbc7"
      },
      "source": [
        "# UNK 토큰 만들기\n",
        "word2idx[\"<oov>\"] = 6\n",
        "print(word2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"n't\": 1, 'lovely': 2, 'love': 3, 'one': 4, 'made': 5, '<oov>': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZODFDuRbXcFv"
      },
      "source": [
        "`<oov>` (unknown UNK) 토큰은 앞쪽 (0번 / 1번) 으로 들어가는 게 일반적임"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkV-26OrXnmd",
        "outputId": "7af81802-23e5-47b7-8f8c-7ddaac733b6a"
      },
      "source": [
        "# 인코딩된 데이터를 저장하기 위한 배열\n",
        "encoded = []\n",
        "\n",
        "for sent in sentences:\n",
        "    # print(sent)\n",
        "\n",
        "    # 임시로 인코딩된 내용을 저장\n",
        "    temp = []\n",
        "\n",
        "    # 단어를 하나씩 꺼내기\n",
        "    for w in sent:\n",
        "        if w in word2idx: # 단어집합에 단어가 있으면\n",
        "            temp.append(word2idx[w]) # 키값에 맞는 정수를 추가\n",
        "        else: # 키값이 없으면\n",
        "            temp.append(word2idx[\"<oov>\"]) # 단어 집합에 없는 단어는 <oov> 토큰의 정수 추가\n",
        "    \n",
        "    encoded.append(temp)\n",
        "\n",
        "print(\"변환 전 : {}\".format(sentences[:5]))\n",
        "print(\"변환 후 : {}\".format(encoded[:5]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "변환 전 : [[\"n't\", 'lovely'], [\"n't\", 'wonderful'], [\"n't\", 'precious'], ['less', 'one', 'minute', 'old'], ['never', 'thought', 'love']]\n",
            "변환 후 : [[1, 2], [1, 6], [1, 6], [6, 4, 6, 6], [6, 6, 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqjCnQz5X0_f"
      },
      "source": [
        "#### [English] Vocab & Integer Encoding ( Tensorflow )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk-wnK50ZChp"
      },
      "source": [
        "# Tensorflow의 Tokenizer 불러오기\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz_7I-5DZZnT"
      },
      "source": [
        "Tokenizer의 fit_on_texts() 함수에 코퍼스(sentences)를 집어 넣으면 바로 빈도수를 기준으로 단어 집합 만들어줌  \n",
        "```python \n",
        "tokenizer.fit_on_texts(sentences)\n",
        "```\n",
        "\n",
        "--> 형태소 분리의 결과가 들어가는데, 이 때 형태소 분리의 결과는 반드시 항상 **2차원 리스트** 여야 함  \n",
        "\n",
        "print()에서 바로 word_index로 확인할 수 있음\n",
        "```python\n",
        "print(tokenizer.word_index)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxr3N_6FZLcB",
        "outputId": "8f6e0303-e426-4b7a-d7cf-dec5d6de843b"
      },
      "source": [
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"n't\": 1, 'lovely': 2, 'love': 3, 'one': 4, 'made': 5, 'life': 6, 'done': 7, 'wonderful': 8, 'precious': 9, 'less': 10, 'minute': 11, 'old': 12, 'never': 13, 'thought': 14, 'making': 15, 'pretty': 16, 'truly': 17, 'angel': 18, 'best': 19, 'boy': 20, 'happy': 21, 'heaven': 22, 'blessed': 23, 'believe': 24, 'god': 25, 'given': 26, 'aisha': 27, 'meaning': 28, 'name': 29, 'londie': 30, 'could': 31, 'without': 32, 'conceived': 33}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u6RI2koZpok",
        "outputId": "04db7ef2-5350-4208-ecc5-6f2e4d571336"
      },
      "source": [
        "# 단어의 빈도수 확인하기 (tokenizer의 word_counts 함수 사용)\n",
        "print(tokenizer.word_counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([(\"n't\", 8), ('lovely', 6), ('wonderful', 1), ('precious', 1), ('less', 1), ('one', 4), ('minute', 1), ('old', 1), ('never', 1), ('thought', 1), ('love', 5), ('making', 1), ('made', 3), ('pretty', 1), ('truly', 1), ('angel', 1), ('best', 1), ('boy', 1), ('happy', 1), ('heaven', 1), ('blessed', 1), ('believe', 1), ('god', 1), ('done', 2), ('given', 1), ('life', 3), ('aisha', 1), ('meaning', 1), ('name', 1), ('londie', 1), ('could', 1), ('without', 1), ('conceived', 1)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waDpLAzsaAQ_",
        "outputId": "33eb284f-fc68-4f7a-c22b-fe5d687a1b0b"
      },
      "source": [
        "# 인코딩 (texts_to_sequences 함수)\n",
        "print(tokenizer.texts_to_sequences(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2], [1, 8], [1, 9], [10, 4, 11, 12], [13, 14, 3], [15, 4, 2], [1, 2, 5, 3], [1, 16], [17, 18, 19], [20, 21], [22, 23], [1, 24, 25, 7], [26, 6, 4], [1, 2, 5, 3], [1, 2], [6, 3], [6, 27], [28, 29], [30, 31, 7], [32, 33, 4], [2, 5, 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7QkAcVwaG86",
        "outputId": "8dc3076a-6eea-4913-c557-23a7dbbe1012"
      },
      "source": [
        "# 디코딩 (sequences_to_texts 함수) - 자동으로 join까지 진행됨\n",
        "print(tokenizer.sequences_to_texts([[1, 2], [1, 9]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"n't lovely\", \"n't precious\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lnmLZFUaYGU"
      },
      "source": [
        "### OOV 및 Padding 설정\n",
        "\n",
        "keras의 Tokenizer에서는 `pad : 0, oov : 1` 로 자동 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PD0B3W7a1aq",
        "outputId": "83d7ccea-d96d-4087-a601-519b17e737de"
      },
      "source": [
        "# 상위 5개의 토큰만 사용하기\n",
        "vocab_size = 5\n",
        "\n",
        "# num_sords : 단어집합 내에서 사용할 단어 개수\n",
        "tokenizer = Tokenizer(num_words=vocab_size+2,    oov_token=\"<oov>\")\n",
        "#   기본 단어 집합에 padding, oov 토큰을 추가적으로 부여해야 하기 때문에 +2\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "print(tokenizer.texts_to_sequences(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 3], [2, 1], [2, 1], [1, 5, 1, 1], [1, 1, 4], [1, 5, 3], [2, 3, 6, 4], [2, 1], [1, 1, 1], [1, 1], [1, 1], [2, 1, 1, 1], [1, 1, 5], [2, 3, 6, 4], [2, 3], [1, 4], [1, 1], [1, 1], [1, 1, 1], [1, 1, 5], [3, 6, 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfDZzx2obphY"
      },
      "source": [
        "1번 토큰 -> `<oov> 토큰`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-irOV9Kpbxi3",
        "outputId": "b92e7c26-3a71-44c7-b63f-63f7f163d890"
      },
      "source": [
        "print(\"OOV 토큰의 인덱스 : {}\".format(tokenizer.word_index[\"<oov>\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OOV 토큰의 인덱스 : 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjsGjPSMck4T"
      },
      "source": [
        "# [Korean] 토큰화 및 정수 인코딩을 Tensorflow로 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv52y_mob6Uu",
        "outputId": "6a6bf4fc-9598-454d-ad27-a508cd0a3b6b"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 155kB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/88/f817ef1af6f794e8f11313dcd1549de833f4599abcec82746ab5ed086686/JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 29.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: beautifulsoup4, colorama, JPype1, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M_dRqTYckOZ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5YosK2yc7af",
        "outputId": "efc277bc-100c-483b-e919-4885307ac813"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ratings_test.txt', <http.client.HTTPMessage at 0x7fd7536513d0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "J3zJygKOdAA1",
        "outputId": "7a6855db-ebb4-4602-b0c7-fb4c1ad8dd94"
      },
      "source": [
        "df_train = pd.read_table(\"ratings_test.txt\", encoding='utf-8')\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6270596</td>\n",
              "      <td>굳 ㅋ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9274899</td>\n",
              "      <td>GDNTOPCLASSINTHECLUB</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8544678</td>\n",
              "      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6825595</td>\n",
              "      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6723715</td>\n",
              "      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                           document  label\n",
              "0  6270596                                                굳 ㅋ      1\n",
              "1  9274899                               GDNTOPCLASSINTHECLUB      0\n",
              "2  8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
              "3  6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
              "4  6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-o0RUyAdOnM",
        "outputId": "731e533a-5581-437b-e6cc-19126323d045"
      },
      "source": [
        "df_train.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        50000 non-null  int64 \n",
            " 1   document  49997 non-null  object\n",
            " 2   label     50000 non-null  int64 \n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 1.1+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMTEtOo1dTAD"
      },
      "source": [
        "데이터 전처리 \n",
        "1. null 값 제거\n",
        "2. 중복 문장 제거\n",
        "3. 특수 문자 및 영어 제거"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfT-WEwodcKU"
      },
      "source": [
        "텍스트 인코딩\n",
        "1. 각 문장별 형태소 분리 \n",
        "2. Tokenizer에 집어 넣기\n",
        "3. 단어 집합 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB6aYH_4m1r7",
        "outputId": "f7c589c1-0997-462e-83e0-40d2449c101d"
      },
      "source": [
        "df_train = df_train.drop_duplicates(subset=['document'])\n",
        "df_train = df_train.dropna(how='any')\n",
        "\n",
        "df_train.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 49157 entries, 0 to 49999\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        49157 non-null  int64 \n",
            " 1   document  49157 non-null  object\n",
            " 2   label     49157 non-null  int64 \n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 1.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "QPfjsITOnEJU",
        "outputId": "f0cbbcae-48bd-4889-dae5-4d958e8d98b0"
      },
      "source": [
        "df_train['document'] = df_train['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6270596</td>\n",
              "      <td>굳 ㅋ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9274899</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8544678</td>\n",
              "      <td>뭐야 이 평점들은 나쁘진 않지만 점 짜리는 더더욱 아니잖아</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6825595</td>\n",
              "      <td>지루하지는 않은데 완전 막장임 돈주고 보기에는</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6723715</td>\n",
              "      <td>만 아니었어도 별 다섯 개 줬을텐데 왜 로 나와서 제 심기를 불편하게 하죠</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                   document  label\n",
              "0  6270596                                        굳 ㅋ      1\n",
              "1  9274899                                                 0\n",
              "2  8544678           뭐야 이 평점들은 나쁘진 않지만 점 짜리는 더더욱 아니잖아      0\n",
              "3  6825595                  지루하지는 않은데 완전 막장임 돈주고 보기에는      0\n",
              "4  6723715  만 아니었어도 별 다섯 개 줬을텐데 왜 로 나와서 제 심기를 불편하게 하죠      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3u1Zbyo5nZ4R",
        "outputId": "7d48540f-ca64-44d2-c536-cbb15681064b"
      },
      "source": [
        "df_train['document'].replace('', np.nan, inplace=True)\n",
        "df_train = df_train.dropna(how='any')\n",
        "df_train.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 48995 entries, 0 to 49999\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        48995 non-null  int64 \n",
            " 1   document  48995 non-null  object\n",
            " 2   label     48995 non-null  int64 \n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 1.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "am8jRlxGnixn",
        "outputId": "106751f4-7bd2-4ed7-b781-2059aea00453"
      },
      "source": [
        "df_train['document'].nunique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48417"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LocbSDiVnqYG",
        "outputId": "e2294b67-a6ce-46a6-fbdc-202e50c7e8f6"
      },
      "source": [
        "df_train.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id          0\n",
              "document    0\n",
              "label       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn0FVsTCpVNg"
      },
      "source": [
        "불용어 처리  \n",
        "\n",
        "일반적으로 한국어에 대한 불용어 처리는 stemming, normalization 적용 후 한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIafe4kantqp"
      },
      "source": [
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-K2o-lMo10c"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "okt = Okt()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "NjwiD6rLo5Vm",
        "outputId": "2feae44b-5c81-4f05-8dd7-70c3a041dcae"
      },
      "source": [
        "X_train = []\n",
        "\n",
        "for sentence in df_train['document']:\n",
        "    temp_X = []\n",
        "    temp_X = okt.morphs(sentence, stem=True, norm=True)\n",
        "    temp_X = [word for word in temp_X if not word in stopwords]\n",
        "\n",
        "    X_train.append(temp_X)\n",
        "\n",
        "X_train[:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-020d1392cdc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'document'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtemp_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtemp_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mokt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtemp_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_X\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/konlpy/tag/_okt.py\u001b[0m in \u001b[0;36mmorphs\u001b[0;34m(self, phrase, norm, stem)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;34m\"\"\"Parse phrase to morphemes.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mphrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/konlpy/tag/_okt.py\u001b[0m in \u001b[0;36mpos\u001b[0;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[1;32m     61\u001b[0m                     \u001b[0mphrase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                     \u001b[0mjpype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                     jpype.java.lang.Boolean(stem)).toArray()\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJ3AqXh6pPYX"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOiiDiGzq4kT"
      },
      "source": [
        "tokenizer.texts_to_sequences(X_train)[:3]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}